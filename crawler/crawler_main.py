import time
import asyncio
from datetime import datetime, timedelta, timezone
import traceback

from wordcloud.wordCloudMaker import make_wordcloud_data
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

from score.trust.trust_pipline import run_trust_pipeline

from crawler.kbs_crawler import kbs_crawl
from crawler.donga_crawler import donga_crawl
from crawler.chosun_crawler import chosun_crawl
from crawler.kmib_crawler import kmib_crawl
from crawler.hani_crawler import hani_crawl
from crawler.naeil_crawl import naeil_crawl
from crawler.everyday_crawler import everyday_crawl
from crawler.hankookilbo_crawler import hankookilbo_crawl

from util.cleaner import clean_articles
from util.elastic import es
from util.logger import Logger
from util.elastic_templates import build_error_doc, build_info_docs
from util.repository import upsert_article

from labeler.create_embeddings import create_embedding
from labeler.categorizer import categorizer

logger = Logger().get_logger(__name__)
KST = timezone(timedelta(hours=9))


def crawl_bigkinds_full():  # ì´ê±´ ê·¸ëƒ¥ ì…€ë ˆë‹ˆì›€í•˜ê¸°ìœ„í•œ ì…‹ì—…
    now_kst = datetime.now(KST).isoformat(timespec="seconds")
    run_id = now_kst[:13].replace("-", "").replace("T", "_")  # ì˜ˆ: 20260107_14
    job_id = "crawl_bigkinds_full"
    t_job0 = time.monotonic()

    print(f"[{now_kst}] ë¹…ì¹´ì¸ì¦ˆ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")

    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")
    # options.add_argument("--headless")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    press_list = ["ë™ì•„ì¼ë³´", "KBS", "í•œê²¨ë ˆ", "ì¡°ì„ ì¼ë³´", "êµ­ë¯¼ì¼ë³´", "ë‚´ì¼ì‹ ë¬¸", "ë§¤ì¼ì‹ ë¬¸", "í•œêµ­ì¼ë³´"]
    all_results = []
    big_error_list = []

    success_list = []

    for press_name in press_list:
        print(f"==== {press_name} í¬ë¡¤ë§ ì‹œì‘ ====")
        press_results = []
        driver.get("https://www.bigkinds.or.kr/v2/news/index.do")
        time.sleep(2)

        # 1) ì–¸ë¡ ì‚¬ ì„ íƒ ê¸°ëŠ¥
        try:
            checkbox = driver.find_element(By.ID, press_name)
            driver.execute_script("arguments[0].click();", checkbox)
        except Exception as e:
            big_error_list.append({
                "error_type": type(e).__name__,
                "error_message": f"{press_name} : {str(e)}"
            })
            # âœ… info_logs: press selection ì‹¤íŒ¨ë„ stage summaryë¡œ ë‚¨ê¹€
            es.index(
                index="info_logs",
                document=build_info_docs(
                    run_id=run_id,
                    job_id=job_id,
                    component="crawler",
                    stage=f"{press_name}_select_end",
                    status="error",
                    duration_ms=None,
                    input_cnt=0,
                    success_cnt=0,
                    failed_cnt=1,
                    message=f"{press_name} press checkbox select failed",
                    error_message=str(e),
                    retryable=True
                )
            )
            continue
        time.sleep(1)

        # 3) ê²€ìƒ‰ í´ë¦­ ê¸°ëŠ¥
        try:
            search_btn = driver.find_element(
                By.CSS_SELECTOR,
                "#search-foot-div > div.foot-btn > button.btn.btn-search.news-search-btn.news-report-search-btn"
            )
            driver.execute_script("arguments[0].click();", search_btn)
        except Exception as e:
            big_error_list.append({
                "error_type": type(e).__name__,
                "error_message": f"{press_name} : {str(e)}"
            })
            # ê²€ìƒ‰ ì‹¤íŒ¨ëŠ” press ë‹¨ìœ„ë¡œ ê³„ì† ì§„í–‰ ê°€ëŠ¥í•˜ë‹ˆ warningìœ¼ë¡œ ë‚¨ê¹€
            es.index(
                index="info_logs",
                document=build_info_docs(
                    run_id=run_id,
                    job_id=job_id,
                    component="crawler",
                    stage=f"{press_name}_search_click_end",
                    status="warn",
                    duration_ms=None,
                    input_cnt=0,
                    success_cnt=0,
                    failed_cnt=1,
                    message=f"{press_name} search click failed",
                    error_message=str(e),
                    retryable=True
                )
            )
            pass
        time.sleep(3)

        # 5) ë‰´ìŠ¤ë¶„ì„ í´ë¦­ ê¸°ëŠ¥
        try:
            analysis_btn = driver.find_element(By.CSS_SELECTOR, "button.step-3-click")
            driver.execute_script("arguments[0].click();", analysis_btn)
        except Exception as e:
            big_error_list.append({
                "error_type": type(e).__name__,
                "error_message": f"{press_name} : {str(e)}"
            })
            # âœ… ë¶„ì„ ë²„íŠ¼ ì‹¤íŒ¨ëŠ” press ë‹¨ìœ„ ì§„í–‰ ë¶ˆê°€ â†’ error
            es.index(
                index="info_logs",
                document=build_info_docs(
                    run_id=run_id,
                    job_id=job_id,
                    component="crawler",
                    stage=f"{press_name}_analysis_click_end",
                    status="error",
                    duration_ms=None,
                    input_cnt=0,
                    success_cnt=0,
                    failed_cnt=1,
                    message=f"{press_name} analysis click failed",
                    error_message=str(e),
                    retryable=True
                )
            )
            continue

        time.sleep(4)

        # 6) í…Œì´ë¸” rows ê°€ì ¸ì˜¤ê¸°
        rows = driver.find_elements(By.CSS_SELECTOR, "#preview-wrap > table > tbody > tr")

        # âœ… BigKinds table parse ë‹¨ê³„ íƒ€ì´ë¨¸(press ë‹¨ìœ„)
        t_press_table0 = time.monotonic()
        parsed_cnt = 0

        for row in rows:
            row_id = row.get_attribute("id")
            row_no = row_id.split('-')[1]

            keywords_raw = driver.find_element(By.CSS_SELECTOR, f'td[id="14-{row_no}"]').text
            feature_raw = driver.find_element(By.CSS_SELECTOR, f'td[id="15-{row_no}"]').text

            org_raw = driver.find_element(By.CSS_SELECTOR, f'td[id="13-{row_no}"]').text
            person_raw = driver.find_element(By.CSS_SELECTOR, f'td[id="11-{row_no}"]').text

            keywords = [k.strip() for k in keywords_raw.split(",") if k.strip()]
            features = [f.strip() for f in feature_raw.split(",") if f.strip()]

            org = [k.strip() for k in org_raw.split(",") if k.strip()]
            person = [k.strip() for k in person_raw.split(",") if k.strip()]

            data = {
                "press": driver.find_element(By.CSS_SELECTOR, f'td[id="2-{row_no}"]').text,
                "article_id": driver.find_element(By.CSS_SELECTOR, f'td[id="0-{row_no}"]').text,
                "upload_date": driver.find_element(By.CSS_SELECTOR, f'td[id="1-{row_no}"]').text,
                "reporter": driver.find_element(By.CSS_SELECTOR, f'td[id="3-{row_no}"]').text,
                "keywords": keywords,
                "features": features,
                "url": driver.find_element(By.CSS_SELECTOR, f'td[id="17-{row_no}"]').text,
                "collected_at": now_kst,
                "entities": {
                    "org": org,
                    "person": person
                }
            }

            all_results.append(data)
            press_results.append(data)
            parsed_cnt += 1

            # BigKinds 1ì°¨ ES ì ì¬(ë®ì–´ì“°ê¸°)
            es.index(
                index="article_data",
                document=data,
                id=data['article_id']
            )

        # âœ… press ë‹¨ìœ„: BigKinds í…Œì´ë¸” íŒŒì‹±/1ì°¨ ì ì¬ ì™„ë£Œ ìš”ì•½
        es.index(
            index="info_logs",
            document=build_info_docs(
                run_id=run_id,
                job_id=job_id,
                component="crawler",
                stage=f"{press_name}_bigkinds_table_end",
                status="ok",
                duration_ms=int((time.monotonic() - t_press_table0) * 1000),
                input_cnt=parsed_cnt,
                success_cnt=parsed_cnt,
                failed_cnt=0,
                message=f"{press_name} parsed {parsed_cnt} rows and indexed to article_data"
            )
        )

        # âœ… press ë‹¨ìœ„: ì–¸ë¡ ì‚¬ë³„ ì›ë¬¸ í¬ë¡¤ëŸ¬ í˜¸ì¶œ
        try:
            t_press_crawl0 = time.monotonic()

            if press_name == "ë™ì•„ì¼ë³´":
                result = asyncio.run(donga_crawl(press_results))
            elif press_name == "KBS":
                result = asyncio.run(kbs_crawl(press_results))
            elif press_name == "í•œê²¨ë ˆ":
                result = asyncio.run(hani_crawl(press_results))
            elif press_name == "ì¡°ì„ ì¼ë³´":
                result = asyncio.run(chosun_crawl(press_results))
            elif press_name == "êµ­ë¯¼ì¼ë³´":
                result = asyncio.run(kmib_crawl(press_results))
            elif press_name == "ë‚´ì¼ì‹ ë¬¸":
                result = asyncio.run(naeil_crawl(press_results))
            elif press_name == "ë§¤ì¼ì‹ ë¬¸":
                result = asyncio.run(everyday_crawl(press_results))
            elif press_name == "í•œêµ­ì¼ë³´":
                result = asyncio.run(hankookilbo_crawl(press_results))
            else:
                result = []

            result = result or []
            success_list.extend(result)

            es.index(
                index="info_logs",
                document=build_info_docs(
                    run_id=run_id,
                    job_id=job_id,
                    component="crawler",
                    stage=f"{press_name}_crawl_end",
                    status="ok",
                    duration_ms=int((time.monotonic() - t_press_crawl0) * 1000),
                    input_cnt=len(press_results),
                    success_cnt=len(result),
                    failed_cnt=max(0, len(press_results) - len(result)),
                    message=f"{press_name} crawler finished"
                )
            )

        except Exception as e:

            # ê¸°ì¡´ error_log ìœ ì§€
            es.index(
                index="error_log",
                document=build_error_doc(
                    message=f"{press_name} í¬ë¡¤ëŸ¬ í˜¸ì¶œ ì‹¤íŒ¨",
                    service_name="crawler",
                    service_environment="dev",
                    pipeline_job="crawl_bigkinds_full",
                    pipeline_step=f"{press_name}_crawl",
                    event_severity=3,
                    exception=e,
                    samples=[{
                        "press": press_name,
                        "traceback": traceback.format_exc()
                    }],
                    tags=["crawler", "bigkinds", press_name]
                )
            )

    es.index(
        index="info_logs",
        document=build_info_docs(
            run_id=run_id,
            job_id=job_id,
            component="crawler",
            stage="bigkinds_collect_and_press_crawl_end",
            status="ok",
            duration_ms=int((time.monotonic() - t_job0) * 1000),
            input_cnt=len(all_results),
            success_cnt=len(success_list),
            failed_cnt=max(0, len(all_results) - len(success_list)),
            message=f"all press done. collected={len(all_results)} success={len(success_list)}"
        )
    )

    driver.quit()

    # ì›Œë“œí´ë¼ìš°ë“œ
    if all_results:
        print("ğŸ“Š ì›Œë“œí´ë¼ìš°ë“œìš© í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘...")
        t0 = time.monotonic()
        asyncio.run(make_wordcloud_data(all_results))
        es.index(
            index="info_logs",
            document=build_info_docs(
                run_id=run_id,
                job_id=job_id,
                component="wordcloud",
                stage="make_wordcloud_data_end",
                status="ok",
                duration_ms=int((time.monotonic() - t0) * 1000),
                input_cnt=len(all_results),
                success_cnt=1,
                failed_cnt=0,
                message="wordcloud data generated"
            )
        )

    id_list = [data["article_id"] for data in all_results]

    logger.info(f"[{now_kst}] ë¹…ì¹´ì¸ì¦ˆ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ. ì´ {len(all_results)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    time.sleep(30)

    print(len(success_list))
    logger.info(f"[{len(id_list)}] ê°œ ê¸°ì‚¬ ì¤‘ . ì´ {len(id_list) - len(success_list)}ê°œ ê²°ì¸¡ì¹˜ ë°œìƒ")

    # ì „ì²˜ë¦¬
    logger.info("ê¸°ì‚¬ ë³¸ë¬¸ ì „ì²˜ë¦¬ ë° ì—…ë°ì´íŠ¸")
    t0 = time.monotonic()
    clean_articles(success_list)
    es.index(
        index="info_logs",
        document=build_info_docs(
            run_id=run_id,
            job_id=job_id,
            component="preprocess",
            stage="clean_articles_end",
            status="ok",
            duration_ms=int((time.monotonic() - t0) * 1000),
            input_cnt=len(success_list),
            success_cnt=len(success_list),
            failed_cnt=0,
            message="clean_articles updated article_data"
        )
    )

    logger.info("ê¸°ì‚¬ë³„ ì„ë² ë”© ìƒì„±")

    if success_list:
        # ì„ë² ë”©
        t0 = time.monotonic()
        create_embedding(success_list)
        es.index(
            index="info_logs",
            document=build_info_docs(
                run_id=run_id,
                job_id=job_id,
                component="embedding",
                stage="create_embedding_end",
                status="ok",
                duration_ms=int((time.monotonic() - t0) * 1000),
                input_cnt=len(success_list),
                success_cnt=len(success_list),
                failed_cnt=0
            )
        )

        # ì¹´í…Œê³ ë¼ì´ì €
        t0 = time.monotonic()
        categorizer(success_list)
        es.index(
            index="info_logs",
            document=build_info_docs(
                run_id=run_id,
                job_id=job_id,
                component="categorizer",
                stage="categorizer_end",
                status="ok",
                duration_ms=int((time.monotonic() - t0) * 1000),
                input_cnt=len(success_list),
                success_cnt=len(success_list),
                failed_cnt=0
            )
        )

        # ì‹ ë¢°ë„
        t0 = time.monotonic()
        run_trust_pipeline(success_list)
        es.index(
            index="info_logs",
            document=build_info_docs(
                run_id=run_id,
                job_id=job_id,
                component="trust",
                stage="trust_pipeline_end",
                status="ok",
                duration_ms=int((time.monotonic() - t0) * 1000),
                input_cnt=len(success_list),
                success_cnt=len(success_list),
                failed_cnt=0
            )
        )

        time.sleep(30)

        # DB upsert
        t0 = time.monotonic()
        upsert_article(success_list)
        es.index(
            index="info_logs",
            document=build_info_docs(
                run_id=run_id,
                job_id=job_id,
                component="db",
                stage="upsert_article_end",
                status="ok",
                duration_ms=int((time.monotonic() - t0) * 1000),
                input_cnt=len(success_list),
                success_cnt=len(success_list),
                failed_cnt=0,
                message="upserted to DB"
            )
        )

    # âœ… ì„¸ì…˜ ìš”ì•½ stage summary
    es.index(
        index="info_logs",
        document=build_info_docs(
            run_id=run_id,
            job_id=job_id,
            component="crawler",
            stage="session_summary_end",
            status="warn" if big_error_list else "ok",
            duration_ms=int((time.monotonic() - t_job0) * 1000),
            input_cnt=len(id_list),
            success_cnt=len(success_list),
            failed_cnt=max(0, len(id_list) - len(success_list)),
            message=f"errors={len(big_error_list)} missing={max(0, len(id_list) - len(success_list))}",
            error_message=f"bigkinds_errors={len(big_error_list)}" if big_error_list else None,
            retryable=False
        )
    )

    # ê¸°ì¡´ error_log ì„¸ì…˜ ìš”ì•½ ìœ ì§€
    if 0 < len(big_error_list) < 20:
        error_doc = build_error_doc(
            message=f"BigKinds í¬ë¡¤ë§ ì¤‘ {len(big_error_list)}ê°œ ì—ëŸ¬ ë°œìƒ",
            service_name="crawler",
            pipeline_run_id=run_id,
            pipeline_job="crawl_bigkinds_full",
            pipeline_step="individual_press",
            event_severity=4,
            event_outcome="warning",
            metrics={
                "error_count": len(big_error_list),
                "success_count": len(success_list),
            },
            samples=big_error_list,
            tags=["crawler", "bigkinds", "session-summary"],
        )
        es.index(index="error_log", document=error_doc)
    elif len(big_error_list) >= 20:
        error_doc = build_error_doc(
            message=f"BigKinds í¬ë¡¤ë§ ì¤‘ {len(big_error_list)}ê°œ ì—ëŸ¬ ë°œìƒ, DOM ê°ì²´ í™•ì¸ í•„ìš”",
            service_name="crawler",
            pipeline_run_id=run_id,
            pipeline_job="crawl_bigkinds_full",
            pipeline_step="individual_press",
            event_severity=3,
            event_outcome="warning",
            metrics={
                "error_count": len(big_error_list),
                "success_count": len(success_list),
            },
            samples=big_error_list,
            tags=["crawler", "bigkinds", "session-summary"],
        )
        es.index(index="error_log", document=error_doc)
    return all_results


if __name__ == '__main__':
    crawl_bigkinds_full()
