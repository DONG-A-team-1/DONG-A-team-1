import httpx
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse
import asyncio # ë¹„ë™ê¸° ì§€ì—°ì„ ìœ„í•´ ì¶”ê°€
from typing import List, Dict, Any
from util.elastic import es

KST = timezone(timedelta(hours=9))
now_kst = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

async def chosun_crawl(bigkinds_data: List[Dict[str, Any]]):
    """
    ë¹…ì¹´ì¸ì¦ˆì—ì„œ ë°›ì€ URL ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡°ì„ ì¼ë³´ ìƒì„¸ ê¸°ì‚¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    429 Too Many Requests ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¹„ë™ê¸° ì§€ì—°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    print(f"ì¡°ì„ ì¼ë³´ ìƒì„¸ í¬ë¡¤ë§ êµ¬ë™ ì‹œì‘:{now_kst}")

    id_list = [data["news_id"] for data in bigkinds_data]
    url_list = [data["url"] for data in bigkinds_data]

    domain = "chosun"
    article_list = []

    # httpxë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° HTTP ìš”ì²­ ì²˜ë¦¬
    async with httpx.AsyncClient(timeout=15.0, headers=HEADERS) as client:

        for news_id, url in zip(id_list, url_list):

            try:
                # ğŸš¨ 429 Too Many Requests ì˜¤ë¥˜ í•´ê²°: ë¹„ë™ê¸° ì§€ì—° ì‹œê°„ ì¶”ê°€ (0.5ì´ˆ)
                await asyncio.sleep(2)

                # ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ ì ‘ì† ë° ë³¸ë¬¸ ì¶”ì¶œ
                resp = await client.get(url)
                resp.raise_for_status() # 4xx, 5xx ì—ëŸ¬ ì‹œ ì˜ˆì™¸ ë°œìƒ

                soup = BeautifulSoup(resp.text, "html.parser")

                # --- ë³¸ë¬¸ ì¶”ì¶œ ---
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in soup.select("div.ad, div.promotion, div.related, div.article-body > :last-child"):
                    tag.decompose()

                paragraphs = soup.select("section.article-body p, div.article-body p")
                full_content = " ".join([p.get_text(strip=True) for p in paragraphs]).strip()
                if not full_content: # ë³¸ë¬¸ì´ ì¶”ì¶œë˜ì§€ ì•Šìœ¼ë©´ AMP ë²„ì „ ì‹œë„ (ì„ íƒ ì‚¬í•­)
                     amp_url = url + "?outputType=amp"
                     amp_resp = await client.get(amp_url)
                     amp_soup = BeautifulSoup(amp_resp.text, "lxml")
                     amp_paragraphs = amp_soup.select("section.article-body p") or amp_soup.select("article p")
                     full_content = " ".join([p.get_text(strip=True) for p in amp_paragraphs]).strip()
                # --- ë³¸ë¬¸ ì¶”ì¶œ ë ---

                # --- ê¸°íƒ€ ì •ë³´ ì¶”ì¶œ ---
                article_name_tag = soup.select_one("h1.article-header__title")
                # ğŸš¨ 'newsTitle' KeyError ë°©ì§€: ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜, ê¸°ë³¸ê°’ ì‚¬ìš©
                article_title = article_name_tag.text.strip() if article_name_tag else "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"


                image_tag = soup.select_one("div.article-body figure img")
                article_img = image_tag.get("src") if image_tag and image_tag.get("src") else None

                es.update(
                    index="article_data",
                    id=news_id,
                    doc={
                        "article_img": article_img,
                    }
                )

                article_raw ={
                    "article_id": news_id,
                    "article_title": article_title,
                    "article_content": full_content 
                }

                es.index(index="article_raw", id=news_id, document=article_raw)

            except httpx.RequestError as e:
                print(f"[ì¡°ì„  ì˜¤ë¥˜] URL ì ‘ê·¼ ì‹¤íŒ¨ ({url}): {e}")
            except Exception as e:
                print(f"[ì¡°ì„  ì˜¤ë¥˜] ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨ ({url}): {e}")

    print(f"ì¡°ì„ ì¼ë³´ {len(article_list)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ.")

