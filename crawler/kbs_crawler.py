import httpx
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse
import asyncio # ÎπÑÎèôÍ∏∞ ÏßÄÏó∞ÏùÑ ÏúÑÌï¥ Ï∂îÍ∞Ä
from typing import List, Dict, Any
import json
import os
import inspect
from util.logger import  build_error_doc

filename = os.path.basename(__file__)
funcname = inspect.currentframe().f_back.f_code.co_name

logger_name = f"{filename}:{funcname}"
now_kst_iso = datetime.now(timezone(timedelta(hours=9))).isoformat()

KST = timezone(timedelta(hours=9))
now_kst = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
BASE_URL = "https://news.kbs.co.kr"
from util.elastic import es

async def kbs_crawl(bigkinds_data: List[Dict[str, Any]]):
    """
    ÎπÖÏπ¥Ïù∏Ï¶àÏóêÏÑú Î∞õÏùÄ URL Î¶¨Ïä§Ìä∏Î•º ÏÇ¨Ïö©ÌïòÏó¨ KBS ÏÉÅÏÑ∏ Í∏∞ÏÇ¨Î•º ÎπÑÎèôÍ∏∞Ï†ÅÏúºÎ°ú ÌÅ¨Î°§ÎßÅÌï©ÎãàÎã§.
    URL Î¶¨Îã§Ïù¥Î†âÏÖò Ïò§Î•ò(302)Î•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ URL Í≤ΩÎ°úÎ•º ÏàòÏ†ïÌï©ÎãàÎã§.
    """
    print(f"KBS ÏÉÅÏÑ∏ ÌÅ¨Î°§ÎßÅ Íµ¨Îèô ÏãúÏûë:{now_kst}")

    id_list = [data["article_id"] for data in bigkinds_data]
    url_list = [data["url"] for data in bigkinds_data]

    article_list = []
    error_list = []
    empty_articles = []

    # httpxÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÎπÑÎèôÍ∏∞ HTTP ÏöîÏ≤≠ Ï≤òÎ¶¨
    async with httpx.AsyncClient(timeout=10.0) as client:
        for article_id, orginal_url in zip(id_list, url_list):
            # üö® Î¶¨Îã§Ïù¥Î†âÏÖò Ïò§Î•ò(302) Ìï¥Í≤∞ Î°úÏßÅ: PC Î≤ÑÏ†Ñ URLÎ°ú Í≤ΩÎ°ú Í∞ïÏ†ú Î≥ÄÍ≤Ω
            # Ïòà: /news/view.do?ncd=...  -> /news/pc/view/view.do?ncd=...
            if "/news/view.do" in orginal_url:
                url = orginal_url.replace("/news/view.do", "/news/pc/view/view.do")
            else:
                url = url # Ïù¥ÎØ∏ Ïò¨Î∞îÎ•∏ ÌòïÏãùÏùº Í≤ΩÏö∞ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©

            try:
                # 0.5Ï¥à ÎπÑÎèôÍ∏∞ ÏßÄÏó∞ Ï∂îÍ∞Ä (ÏÑúÎ≤Ñ Î∂ÄÌïò Í∞êÏÜå)
                await asyncio.sleep(0.5)

                resp = await client.get(url)
                resp.raise_for_status() # HTTP Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÎ©¥ ÏòàÏô∏ Î∞úÏÉù

                soup = BeautifulSoup(resp.text, "html.parser")

                # --- Í∏∞ÏÇ¨ Î≥∏Î¨∏ Ï∂îÏ∂ú ---
                content = soup.select_one("div#cont_newstext")
                article_content = content.get_text(strip=True) if content else None

                # --- ÎÇòÎ®∏ÏßÄ Ï†ïÎ≥¥ Ï∂îÏ∂ú ---
                # 'data["newsTitle"]'Ïù¥ ÏïÑÎãå ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄÏóêÏÑú Ï∂îÏ∂úÌïòÍ±∞ÎÇò, ÏïàÏ†ÑÌïú Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
                article_title = soup.select_one("div.view-headline h4").text.strip() if soup.select_one(
                    "div.view-headline h4") else None

                news_img = soup.select_one("div#element-image img")
                article_img = news_img["src"] if news_img and news_img.get("src") else None

                es.update(
                    index="article_data",
                    id=article_id,
                    doc={
                        "article_img": article_img
                    }
                )

                article_raw ={
                    "article_id": article_id,
                    "article_title": article_title,
                    "article_content": article_content,
                    "collected_at": now_kst_iso
                }

            except Exception as e:
                error_list.append({
                    "error_url": url,
                    "error_type": type(e).__name__,
                    "error_message": f"{str(e)}"
                })
                continue

            null_count = sum(1 for v in article_raw.values() if v in (None, "", []))
            if null_count == 0:
                es.index(index="article_raw", id=article_id, document=article_raw)
            else:
                empty_articles.append({
                    "article_id": article_id
                })

        # ÏóêÎü¨ Î°úÍ∑∏ ÏóÖÎ°úÎìú
        if len(error_list) > 0:
            error_doc = build_error_doc(
                message=f"{len(error_list)}Í∞ú ÏóêÎü¨ Î∞úÏÉù",
                samples=error_list
            )
            es.index(index="error_log", document=error_doc)

        if len(empty_articles) > 0:
            es.index(
                index="error_log",
                document=build_error_doc(
                    message=f"{len(empty_articles)}Í∞ú Í≤∞Ï∏°Ïπò Î∞úÏÉù",
                    samples=empty_articles
                )
            )
    print("==========KBS ÌÅ¨Î°§ÎßÅ Ï¢ÖÎ£å==========")