import httpx
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse
import asyncio # ë¹„ë™ê¸° ì§€ì—°ì„ ìœ„í•´ ì¶”ê°€
from typing import List, Dict, Any
import json
import os
import inspect


filename = os.path.basename(__file__)
funcname = inspect.currentframe().f_back.f_code.co_name

logger_name = f"{filename}:{funcname}"
now_kst_iso = datetime.now(timezone(timedelta(hours=9))).isoformat()

KST = timezone(timedelta(hours=9))
now_kst = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
BASE_URL = "https://news.kbs.co.kr"
from util.elastic import es

async def kbs_crawl(bigkinds_data: List[Dict[str, Any]]):
    """
    ë¹…ì¹´ì¸ì¦ˆì—ì„œ ë°›ì€ URL ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ KBS ìƒì„¸ ê¸°ì‚¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    URL ë¦¬ë‹¤ì´ë ‰ì…˜ ì˜¤ë¥˜(302)ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ URL ê²½ë¡œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    print(f"KBS ìƒì„¸ í¬ë¡¤ë§ êµ¬ë™ ì‹œì‘:{now_kst}")

    id_list = [data["news_id"] for data in bigkinds_data]
    url_list = [data["url"] for data in bigkinds_data]

    domain = "kbs"
    article_list = []

    # httpxë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° HTTP ìš”ì²­ ì²˜ë¦¬
    async with httpx.AsyncClient(timeout=10.0) as client:
        for news_id, orginal_url in zip(id_list, url_list):
            # ğŸš¨ ë¦¬ë‹¤ì´ë ‰ì…˜ ì˜¤ë¥˜(302) í•´ê²° ë¡œì§: PC ë²„ì „ URLë¡œ ê²½ë¡œ ê°•ì œ ë³€ê²½
            # ì˜ˆ: /news/view.do?ncd=...  -> /news/pc/view/view.do?ncd=...
            if "/news/view.do" in orginal_url:
                url = orginal_url.replace("/news/view.do", "/news/pc/view/view.do")
            else:
                url = url # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©

            try:
                # 0.5ì´ˆ ë¹„ë™ê¸° ì§€ì—° ì¶”ê°€ (ì„œë²„ ë¶€í•˜ ê°ì†Œ)
                await asyncio.sleep(0.5)

                resp = await client.get(url)
                resp.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë°œìƒ

                soup = BeautifulSoup(resp.text, "html.parser")


                # --- ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ---
                article_content = soup.select_one("div.detail-body")
                content = article_content.get_text(strip=True) if article_content else None

                # --- ë‚˜ë¨¸ì§€ ì •ë³´ ì¶”ì¶œ ---
                # 'data["newsTitle"]'ì´ ì•„ë‹Œ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜, ì•ˆì „í•œ ê¸°ë³¸ê°’ ì‚¬ìš©
                article_title = soup.select_one("div.detail-title h2").text.strip() if soup.select_one("div.detail-title h2") else None

                news_img = soup.select_one("div.detail-visual img")
                article_img = BASE_URL + news_img["src"] if news_img and news_img.get("src") else None

                es.update(
                    index="article_data",
                    id=news_id,
                    doc={
                        "article_img": article_img
                    }
                )

                article_raw ={
                    "article_id": news_id,
                    "article_title": article_title,
                    "article_content": content
                }

                error_doc = {
                "@timestamp": now_kst_iso,
                "log": {
                    "level": "ERROR",
                    "logger": logger_name
                },
                "message": f"{news_id}ê²°ì¸¡ì¹˜ ì¡´ì¬, url :{url}"
                }

                null_count = 0
                for v in article_raw.values():
                    if v in (None, "", []):
                        null_count += 1
                if null_count >= 1:
                    es.create(index="error_log", id=news_id, document=error_doc)  
                    continue
                else:
                    es.index(index="article_raw", id=news_id, document=article_raw)

            except httpx.RequestError as e:
                print(f"[KBS ì˜¤ë¥˜] URL ì ‘ê·¼ ì‹¤íŒ¨ ({url}): {e}")
            except Exception as e:
                # 'newsTitle' KeyError ë°©ì§€ë¥¼ ìœ„í•´ data.get("newsTitle", "...") ëŒ€ì‹  ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ë„ë¡ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
                print(f"[KBS ì˜¤ë¥˜] ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨ ({url}): {e}")

    print(f"KBS {len(article_list)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ.")
