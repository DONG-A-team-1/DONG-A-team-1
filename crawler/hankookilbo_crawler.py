import httpx
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import asyncio # ë¹„ë™ê¸° ì§€ì—°ì„ ìœ„í•´ ì¶”ê°€
from typing import List, Dict, Any
import os
import inspect
from util.elastic_templates import build_error_doc

filename = os.path.basename(__file__)
funcname = inspect.currentframe().f_back.f_code.co_name

logger_name = f"{filename}:{funcname}"
now_kst_iso = datetime.now(timezone(timedelta(hours=9))).isoformat()

KST = timezone(timedelta(hours=9))
now_kst = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
BASE_URL = "https://www.hankookilbo.com/"
from util.elastic import es

async def hankookilbo_crawl(bigkinds_data: List[Dict[str, Any]]):
    """
    ë¹…ì¹´ì¸ì¦ˆì—ì„œ ë°›ì€ URL ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ KBS ìƒì„¸ ê¸°ì‚¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    URL ë¦¬ë‹¤ì´ë ‰ì…˜ ì˜¤ë¥˜(302)ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ URL ê²½ë¡œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    print(f"í•œêµ­ì¼ë³´ ìƒì„¸ í¬ë¡¤ë§ êµ¬ë™ ì‹œì‘:{now_kst}")

    id_list = [data["article_id"] for data in bigkinds_data]
    url_list = [data["url"] for data in bigkinds_data]

    article_list = []
    error_list = []
    empty_articles = []

    # httpxë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° HTTP ìš”ì²­ ì²˜ë¦¬
    async with httpx.AsyncClient(timeout=10.0) as client:
        for article_id, orginal_url in zip(id_list, url_list):
            # ğŸš¨ ë¦¬ë‹¤ì´ë ‰ì…˜ ì˜¤ë¥˜(302) í•´ê²° ë¡œì§: PC ë²„ì „ URLë¡œ ê²½ë¡œ ê°•ì œ ë³€ê²½
            # ì˜ˆ: /news/view.do?ncd=...  -> /news/pc/view/view.do?ncd=...
            if "/news/view.do" in orginal_url:
                url = orginal_url.replace("/news/view.do", "/news/pc/view/view.do")
            else:
                url = orginal_url # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©

            try:
                # 0.5ì´ˆ ë¹„ë™ê¸° ì§€ì—° ì¶”ê°€ (ì„œë²„ ë¶€í•˜ ê°ì†Œ)
                await asyncio.sleep(0.5)

                resp = await client.get(url)
                resp.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë°œìƒ

                soup = BeautifulSoup(resp.text, "html.parser")

                # --- ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ---
                content_div = soup.select_one("div.end-body div.col-main")

                if content_div:
                    # 2. ë¶€ëª¨ div ì•ˆì˜ ëª¨ë“  p íƒœê·¸ë¥¼ ì°¾ìŒ
                    p_tags = content_div.select("p")

                    # 3. ê° p íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ë½‘ì•„ ì¤„ë°”ê¿ˆ(\n)ìœ¼ë¡œ í•©ì¹¨
                    # (ë‚´ìš©ì´ ì—†ëŠ” ë¹ˆ p íƒœê·¸ëŠ” ì œì™¸)
                    article_content = "\n".join([p.get_text(strip=True) for p in p_tags if p.get_text(strip=True)])
                else:
                    article_content = None

                # --- ë‚˜ë¨¸ì§€ ì •ë³´ ì¶”ì¶œ ---
                # 'data["newsTitle"]'ì´ ì•„ë‹Œ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜, ì•ˆì „í•œ ê¸°ë³¸ê°’ ì‚¬ìš©
                article_title = soup.select_one("div.end-top div.col-main h1.title").text.strip() if soup.select_one(
                    "div.end-top div.col-main h1.title") else None

                news_img = soup.select_one("div.img-box img")
                article_img = news_img["src"] if news_img and news_img.get("src") else None
                
                #base64 ë°©ì§€
                if len(article_img) > 500:
                    article_img = None
                    
                es.update(
                    index="article_data",
                    id=article_id,
                    doc={
                        "article_img": article_img
                    }
                )

                article_raw ={
                    "article_id": article_id,
                    "article_title": article_title,
                    "article_content": article_content,
                    "collected_at": now_kst_iso
                }

            except Exception as e:
                error_list.append({
                    "error_url": url,
                    "error_type": type(e).__name__,
                    "error_message": f"{str(e)}"
                })
                continue

            null_count = sum(1 for v in article_raw.values() if v in (None, "", []))
            if null_count == 0:
                es.index(index="article_raw", id=article_id, document=article_raw)
            else:
                empty_articles.append({
                    "article_id": article_id
                })
                es.delete(index="article_data", id=article_id)

        # ì—ëŸ¬ ë¡œê·¸ ì—…ë¡œë“œ
        if len(error_list) > 0:
            error_doc = build_error_doc(
                message=f"{len(error_list)}ê°œ ì—ëŸ¬ ë°œìƒ",
                samples=error_list
            )
            es.index(index="error_log", document=error_doc)

        if len(empty_articles) > 0:
            es.index(
                index="error_log",
                document=build_error_doc(
                    message=f"{len(empty_articles)}ê°œ ê²°ì¸¡ì¹˜ ë°œìƒ",
                    samples=empty_articles
                )
            )
    empty_ids = {x["article_id"] for x in empty_articles}
    result = list(set(id_list) - empty_ids)
    print(f"==== ì¡°ì„ ì¼ë³´ ìƒì„¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(result)}ê°œ ì„±ê³µ====")
    return result