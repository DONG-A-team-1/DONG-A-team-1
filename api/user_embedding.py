from datetime import datetime, timezone, timedelta
from util.elastic import es
from util.database import SessionLocal
from sqlalchemy import text
from util.logger import Logger

import numpy as np
import json
import random

"""
[ì•ˆì „ ë²„ì „ íŒ¨ì¹˜ í¬ì¸íŠ¸ ìš”ì•½]

1. ì¶”ì²œ ì…ë ¥ ê¸°ì‚¬ = status=5 ë§Œ ì‚¬ìš©
2. article_label í•­ìƒ .get() ì ‘ê·¼
3. íŠ¸ë Œë“œ / ì‹ ë¢°ë„ ì ìˆ˜ None / ëˆ„ë½ ë°©ì–´
4. ìƒˆ í™˜ê²½ì—ì„œë„ KeyError ë°œìƒ ë¶ˆê°€
"""

logger = Logger().get_logger(__name__)
KST = timezone(timedelta(hours=9))


def _soft_shuffle_topk(ranked, top_k=12, strength=1.0):
    """
    top_k: ì„ì„ ìƒìœ„ êµ¬ê°„
    strength: 0.0ì´ë©´ ê±°ì˜ ì ìˆ˜ìˆœ, 1.0~2.0ì´ë©´ ë³€ë™ì„± ì¦ê°€
              (ì ìˆ˜ ì°¨ì´ê°€ í´ìˆ˜ë¡ ìƒìœ„ê°€ ë” ìì£¼ ìœ ì§€ë¨)
    """
    if len(ranked) <= 1:
        return ranked

    k = min(top_k, len(ranked))
    head = ranked[:k]
    tail = ranked[k:]

    # ì ìˆ˜ ë†’ì€ ì• ê°€ ì•ì— ë” ìì£¼ ì˜¤ë„ë¡:
    # 1) headì—ì„œ í•˜ë‚˜ ë½‘ê³ 
    # 2) ë½‘ì€ ì•  ì œê±°
    # 3) ë°˜ë³µ (without replacement)
    # ê°€ì¤‘ì¹˜ëŠ” score^strength ì‚¬ìš©
    out = []
    pool = head[:]
    while pool:
        weights = [(max(1, x["final_score"]) ** strength) for x in pool]
        pick = random.choices(pool, weights=weights, k=1)[0]
        out.append(pick)
        pool.remove(pick)

    return out + tail

# -------------------------------------------------
# ìœ ì € ì„ë² ë”© ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ìœ ì§€)
# -------------------------------------------------
def update_user_embedding(user_id):
    db = SessionLocal()

    article_row = db.execute(
        text("""
            SELECT sd.article_id, ps.preference_score
            FROM session_data sd
            JOIN preference_score ps ON ps.session_id = sd.session_id
            WHERE sd.user_id = :uid
            ORDER BY ps.occurred_at DESC
            LIMIT 1
        """),
        {"uid": user_id}
    ).fetchone()

    if not article_row:
        return

    article_id, preference_score = article_row
    preference_score = float(preference_score)

    resp = es.search(
        index="article_data",
        body={
            "_source": ["article_embedding"],
            "query": {"term": {"article_id": article_id}}
        }
    )

    hits = resp.get("hits", {}).get("hits", [])
    if not hits:
        return

    article_embedding = hits[0]["_source"].get("article_embedding")
    if not article_embedding or len(article_embedding) != 768:
        raise RuntimeError(f"Invalid article_embedding article_id={article_id}")

    old_emb = np.asarray(article_embedding, dtype=np.float32)

    resp = es.search(
        index="user_embeddings",
        body={
            "_source": ["embedding"],
            "query": {"term": {"user_id": user_id}}
        }
    )

    hits = resp.get("hits", {}).get("hits", [])

    if not hits:
        es.index(
            index="user_embeddings",
            id=user_id,
            document={
                "user_id": user_id,
                "embedding": old_emb.tolist(),
                "updated_at": datetime.now(KST)
            }
        )
        # [LOG-4A] ìµœì´ˆ ê°œì¸í™” ì‹œì 
        logger.info(f"[EMB UPDATE] user_id={user_id} CREATE")
        return

    user_embedding = hits[0]["_source"].get("embedding")
    if not user_embedding or len(user_embedding) != 768:
        raise RuntimeError(f"Invalid user_embedding user_id={user_id}")

    new_emb = np.asarray(user_embedding, dtype=np.float32)
    updated_embedding = 0.9 * old_emb + 0.1 * preference_score * new_emb

    es.update(
        index="user_embeddings",
        id=user_id,
        body={
            "doc": {
                "embedding": updated_embedding.tolist(),
                "updated_at": datetime.now(KST)
            }
        }
    )
    # [LOG-4B] ê°œì¸í™” ëˆ„ì  ë°˜ì˜
    logger.info(f"[EMB UPDATE] user_id={user_id} UPDATE")


# -------------------------------------------------
# ìœ ì € ê¸°ì‚¬ ì¡°íšŒ (ê¸°ì¡´ ìœ ì§€)
# -------------------------------------------------
def user_articles(user_id):
    resp = es.search(
        index="user_embeddings",
        body={
            "_source": ["embedding"],
            "query": {"term": {"user_id": user_id}}
        }
    )

    hits = resp.get("hits", {}).get("hits", [])
    if not hits:
        return []

    query_vec = hits[0]["_source"]["embedding"]

    res = es.search(
        index="article_data",
        size=20,
        knn={
            "field": "article_embedding",
            "query_vector": query_vec,
            "k": 1000,
            "num_candidates": 2000,
            "filter": [
                {"term": {"status": 5}},
                {"range": {"collected_at": {"gte": "now-3d"}}}
            ]
        },
        _source=["article_id", "article_title", "collected_at"]
    )

    return [
        {
            "article_id": h["_source"].get("article_id"),
            "title": h["_source"].get("article_title"),
            "score": h["_score"],
            "collected_at": h["_source"].get("collected_at"),
        }
        for h in res.get("hits", {}).get("hits", [])
    ]

def get_similar_users_mean_embedding(
    vec: list,
    top_k: int = 5,
):
    if not vec or len(vec) != 768:
        return None

    # 2. ìœ ì‚¬ ìœ ì € kNN ê²€ìƒ‰ (ë³¸ì¸ ì œì™¸ëŠ” embedding ë™ì¼ì„±ìœ¼ë¡œ ê°„ì ‘ ì²˜ë¦¬)
    res = es.search(
        index="user_embeddings",
        size=top_k,
        knn={
            "field": "embedding",
            "query_vector": vec,
            "k": top_k,
            "num_candidates": 100,
        },
        _source=["embedding"],
    )

    hits = res.get("hits", {}).get("hits", [])
    if not hits:
        return None

    emb_list = []

    for h in hits:
        emb = (h.get("_source") or {}).get("embedding")
        if not emb or len(emb) != 768:
            continue

        # 3. ğŸ”¥ ë³¸ì¸ embedding ì œì™¸ (ì™„ì „ ë™ì¼ ë²¡í„° ë°©ì–´)
        if np.allclose(emb, vec, atol=1e-6):
            continue
        emb_list.append(np.asarray(emb, dtype=np.float32))

    if not emb_list:
        return None

    # 4. í‰ê·  ì„ë² ë”© ê³„ì‚° + ì •ê·œí™”
    mean_emb = np.mean(emb_list, axis=0)
    norm = np.linalg.norm(mean_emb)

    if norm == 0:
        return None
    return mean_emb / norm

def dedupe_hits(base_hits: list, item_hits: list) -> list:
    """
    base_hits + item_hits ë¥¼ article_id ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    - base_hits ìš°ì„ 
    - item_hitsëŠ” baseì— ì—†ëŠ” ê¸°ì‚¬ë§Œ ì¶”ê°€
    - ì…ë ¥ hit êµ¬ì¡° ê·¸ëŒ€ë¡œ ìœ ì§€ (_score í¬í•¨)

    return: deduped hits list
    """
    seen = set()
    merged = []

    # 1) base í›„ë³´ ë¨¼ì €
    for h in base_hits or []:
        src = h.get("_source", {})
        aid = src.get("article_id")
        if not aid:
            continue
        if aid in seen:
            continue

        seen.add(aid)
        merged.append(h)

    # 2) item-based í›„ë³´ ì¶”ê°€
    for h in item_hits or []:
        src = h.get("_source", {})
        aid = src.get("article_id")
        if not aid:
            continue
        if aid in seen:
            continue

        seen.add(aid)
        merged.append(h)

    return merged


def recommend_articles(user_id: str, limit: int = 20,random: bool = False):
    """
    ìœ ì €ë³„ ì¶”ì²œ ê¸°ì‚¬ ìƒì„± (ì•ˆì „ ë²„ì „)

    - user_embeddings ì¸ë±ìŠ¤ ì—†ì„ ë•Œë„ ì ˆëŒ€ ì—ëŸ¬ ì•ˆ ë‚¨
    - status=5 ê¸°ì‚¬ë§Œ ì¶”ì²œ
    - article_label ëˆ„ë½ ì™„ì „ ë°©ì–´
    """
    # -------------------------------------------------
    # 1. ìœ ì € ì„ë² ë”© ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ğŸ”¥ í•µì‹¬)
    # -------------------------------------------------
    if not es.indices.exists(index="user_embeddings"):
        has_user_embedding = False
        user_hits = []
    else:
        resp = es.search(
            index="user_embeddings",
            body={
                "_source": ["embedding"],
                "query": {"term": {"user_id": user_id}}
            }
        )
        user_hits = resp.get("hits", {}).get("hits", [])
        has_user_embedding = len(user_hits) > 0
        # [LOG-1] ì½œë“œìŠ¤íƒ€íŠ¸ / ê°œì¸í™” ë¶„ê¸° í™•ì¸
        logger.info(
            f"[RECOMMEND] user_id={user_id} has_user_embedding={has_user_embedding}"
        )
    # -------------------------------------------------
    # 2. í›„ë³´ ê¸°ì‚¬ ì¡°íšŒ
    # -------------------------------------------------
    if has_user_embedding:
        query_vec = user_hits[0]["_source"]["embedding"]
        res_base = es.search(
            index="article_data",
            size=100,
            knn={
                "field": "article_embedding",
                "query_vector": query_vec,
                "k": 100,
                "num_candidates": 500,
                "filter": [
                    {"term": {"status": 5}},
                    {"range": {"collected_at": {"gte": "now-3d"}}}
                ]
            },
            _source=[
                "article_id",
                "article_title",
                "article_label",
                "collected_at",
                "article_img",
                "article_content",    #  ë³¸ë¬¸  í•´ì • ê°œì¸í™” í˜ì´ì§€ ì¶”ê°€
                "reporter",           #  ê¸°ìëª…
                "press",              #  ì–¸ë¡ ì‚¬
                "upload_date"         #  ì—…ë¡œë“œ ë‚ ì§œ
            ]
        )

        similar_user_vec = get_similar_users_mean_embedding(query_vec)
        res_item = es.search(
            index="article_data",
            size=200,
            knn={
                "field": "article_embedding",
                "query_vector": similar_user_vec,
                "k": 200,
                "num_candidates": 1000,
                "filter": [
                    {"term": {"status": 5}},
                    {"range": {"collected_at": {"gte": "now-3d"}}}
                ]
            },
            _source=[
                "article_id",
                "article_title",
                "article_label",
                "collected_at",
                "article_img",
                "article_content", #í•´ì • ì¶”ê°€
                "reporter",
                "press",
                "upload_date"
            ]
        )

        base_hits = res_base.get("hits", {}).get("hits", [])
        item_hits = res_item.get("hits", {}).get("hits", [])

        # hits = dedupe_hits(base_hits, item_hits)
        hits = base_hits
        if not hits:
            return []
    else:
        res = es.search(
            index="article_data",
            size=100,
            query={
                "bool": {
                    "must": [
                        {"term": {"status": 5}},
                        {"range": {"collected_at": {"gte": "now-3d"}}}
                    ]
                }
            },
            sort=[{"article_label.trend_score": {"order": "desc"}}],
            _source=[
                "article_id",
                "article_title",
                "article_label",
                "collected_at",
                "article_img",
                "article_content", # í•´ì • ì¶”ê°€
                "reporter",
                "press",
                "upload_date"
            ]
        )

        hits = res.get("hits", {}).get("hits", [])
        if not hits:
            return []
    filtered_hits = []

    for h in hits:
        src = h.get("_source", {})
        title = src.get("article_title", "").strip()

        # 1. ì œëª© ë„ˆë¬´ ì§§ì€ ê²½ìš° ì œê±°
        if len(title) < 12:
            continue

        filtered_hits.append(h)

    hits = filtered_hits
    # [LOG-2] ì¶”ì²œ í›„ë³´ ìˆ˜ í™•ì¸ (kNN / í•„í„° ì •ìƒ ì—¬ë¶€)
    logger.info(
        f"[RECOMMEND] user_id={user_id} candidate_hits={len(hits)}"
    )

    if not hits:
        return []
    # -------------------------------------------------
    # 3. ì ìˆ˜ ë²”ìœ„ ê³„ì‚°
    # -------------------------------------------------
    trend_scores = []
    trust_scores = []

    for h in hits:
        label = h["_source"].get("article_label", {})
        trend_scores.append(label.get("trend_score", 0.0))
        trust_scores.append(label.get("article_trust_score", 0.0))

    trend_min, trend_max = min(trend_scores), max(trend_scores)
    trust_min, trust_max = min(trust_scores), max(trust_scores)

    if has_user_embedding:
        emb_scores = [h["_score"] for h in hits]
        emb_min, emb_max = min(emb_scores), max(emb_scores)
    else:
        emb_min = emb_max = None

    def normalize(v, mn, mx):
        if mx == mn:
            return 0.0
        return (v - mn) / (mx - mn)

    # -------------------------------------------------
    # 4. ìµœì¢… ì ìˆ˜ ê³„ì‚°
    # -------------------------------------------------
    ranked = []

    for h in hits:
        src = h["_source"]
        label = src.get("article_label", {})

        # ğŸ”¥ íŠ¸ë Œë“œ ì ìˆ˜: í•­ìƒ 0~1 ë²”ìœ„ â†’ 100ë°°
        raw_trend = label.get("trend_score")
        if raw_trend is None:
            trend_score = 0
        else:
            trend_score = round(float(raw_trend) * 100)  # 0.74 â†’ 74

        # ğŸ”¥ ì‹ ë¢°ë„ ì ìˆ˜: ì´ë¯¸ 1~100 ë²”ìœ„ â†’ ë°˜ì˜¬ë¦¼ë§Œ
        raw_trust = label.get("article_trust_score")
        if raw_trust is None:
            trust_score = 0
        else:
            trust_score = round(float(raw_trust))  # 68.57 â†’ 69
        #----------------------------------------------------í•´ì •
        trend = normalize(label.get("trend_score", 0.0), trend_min, trend_max)
        trust = normalize(label.get("article_trust_score", 0.0), trust_min, trust_max)

        if has_user_embedding:
            emb = normalize(h["_score"], emb_min, emb_max)
            final_raw = 0.6 * emb + 0.2 * trend + 0.2 * trust
        else:
            final_raw = 0.7 * trend + 0.3 * trust

        ranked.append({
            "article_id": src.get("article_id"),
            "title": src.get("article_title", ""),
            "article_img": src.get("article_img"),
            "final_score": int(round(final_raw * 100)),
            "collected_at": src.get("collected_at"),
            # ì¶”ê°€ í•„ë“œ í•´ì • ì¶”ê°€
            "content": src.get("article_content", ""),
            "reporter": src.get("reporter", ""),
            "press": src.get("press", ""),
            "category": label.get("category", "ê¸°íƒ€"),
            "upload_date": src.get("upload_date"),
            # âœ… ì ìˆ˜ ì¶”ê°€
            "trend_score": label.get("trend_score", 0.0),
            "trust_score": label.get("article_trust_score", 0.0),
        })

    ranked.sort(key=lambda x: x["final_score"], reverse=True)
    # [LOG-3] ìµœì¢… ì¶”ì²œ ê²°ê³¼ (ì²´ê° í™•ì¸ìš© í•µì‹¬ ë¡œê·¸)
    logger.info(
        f"[RECOMMEND RESULT] user_id={user_id} "
        f"top_ids={[r['article_id'] for r in ranked[:limit]]}"
    )
    if not random:
        return ranked[:limit]
    else:
        return _soft_shuffle_topk(ranked, top_k=12, strength=1.2)[:limit]


if __name__ == "__main__":
    logger.info(
        json.dumps(
            recommend_articles("test_user"),
            ensure_ascii=False,
            indent=2
        )
    )